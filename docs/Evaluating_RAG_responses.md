# Evaluating RAG (Retrieval-Augmented Generation) Model Responses

This document outlines methods for systematically evaluating the responses generated by a Retrieval-Augmented Generation (RAG) model. These approaches include offline evaluation techniques, real-time monitoring, and debugging strategies to ensure the model provides accurate and contextually relevant answers.

---

## **1. Evaluation Goals**

The primary goals of evaluating a RAG model response are:

- **Accuracy**: Ensure the generated response aligns with the retrieved documents and query.
- **Relevance**: Validate that the retrieved documents are appropriate and relevant to the query.
- **Grounding**: Confirm that the response is grounded in the retrieved context and does not hallucinate unsupported claims.
- **Monitoring**: Identify misaligned or low-quality responses in real time.

---

## **2. Offline Evaluation**

### **2.1 Logging Query and Response**
Log queries and their corresponding responses for offline analysis.

#### **Implementation**
```python
import logging

# Set up logging
logging.basicConfig(filename="query_logs.log", level=logging.INFO)

def log_query_and_answer(chain, query):
    answer = chain.run({"question": query})
    logging.info(f"Query: {query}")
    logging.info(f"Answer: {answer}")
    return query, answer

# Example Usage
query = "What is quantum mechanics?"
query, answer = log_query_and_answer(qa_chain, query)
```

---

### **2.2 Save Logs to a Database**
Store query and response pairs in a database for detailed offline analysis.

#### **Implementation with SQLite**
```python
import sqlite3

# Create database connection
conn = sqlite3.connect("query_logs.db")
cursor = conn.cursor()

# Create table for storing logs
cursor.execute("""
CREATE TABLE IF NOT EXISTS query_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query TEXT,
    answer TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
)
""")
conn.commit()

def save_query_and_answer(chain, query):
    answer = chain.run({"question": query})
    cursor.execute("INSERT INTO query_logs (query, answer) VALUES (?, ?)", (query, answer))
    conn.commit()
    return query, answer

# Example Usage
query = "What is quantum mechanics?"
query, answer = save_query_and_answer(qa_chain, query)
```

---

### **2.3 Evaluate Response Similarity**
Calculate the cosine similarity between the query and the generated response to evaluate their alignment.

#### **Implementation with SentenceTransformers**
```python
from sentence_transformers import SentenceTransformer, util

# Initialize embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

def evaluate_similarity(query, answer, threshold=0.7):
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)
    answer_embedding = embedding_model.encode(answer, convert_to_tensor=True)
    similarity = util.pytorch_cos_sim(query_embedding, answer_embedding).item()
    
    if similarity < threshold:
        print(f"Misaligned response detected! Similarity: {similarity:.2f}")
    else:
        print(f"Response aligned. Similarity: {similarity:.2f}")
    return similarity

# Example Usage
query = "What is quantum mechanics?"
answer = "Quantum mechanics explains physical phenomena at microscopic scales."
evaluate_similarity(query, answer)
```

---

## **3. Real-Time Monitoring and Alerts**

### **3.1 Monitoring with Logging**
Track and log misaligned responses during runtime.

#### **Implementation**
```python
import logging

# Set up logging for real-time alerts
logging.basicConfig(filename="alerts.log", level=logging.WARNING)

def monitor_alignment(chain, query, threshold=0.7):
    answer = chain.run({"question": query})
    similarity = evaluate_similarity(query, answer, threshold)
    
    if similarity < threshold:
        logging.warning(f"ALERT: Misaligned response for query: {query}")
        print(f"ALERT: Misaligned response detected for query: {query}")
    return query, answer

# Example Usage
query = "What is quantum mechanics?"
query, answer = monitor_alignment(qa_chain, query)
```

---

### **3.2 Alerts via Email**
Send alerts when responses are detected as misaligned.

#### **Implementation**
```python
import smtplib

def send_email_alert(query, answer, similarity):
    subject = "ALERT: Misaligned Response Detected"
    body = f"Query: {query}\nAnswer: {answer}\nSimilarity Score: {similarity:.2f}"
    message = f"Subject: {subject}\n\n{body}"

    # Configure SMTP
    server = smtplib.SMTP("smtp.example.com", 587)
    server.starttls()
    server.login("your_email@example.com", "your_password")
    server.sendmail("your_email@example.com", "alert_recipient@example.com", message)
    server.quit()

# Example Usage
query = "What is quantum mechanics?"
answer = "Some incorrect response."
similarity = 0.5
send_email_alert(query, answer, similarity)
```

---

## **4. Debugging Retrieval Issues**

### **4.1 Inspect Retrieved Documents**
Log retrieved documents to verify relevance.

#### **Implementation**
```python
def inspect_retrieval(retriever, query, k=5):
    retrieved_docs = retriever.similarity_search(query, k=k)
    print("Retrieved Documents:")
    for doc in retrieved_docs:
        print(doc.page_content)
    return retrieved_docs

# Example Usage
query = "What is quantum mechanics?"
inspect_retrieval(retriever, query)
```

---

### **4.2 Evaluate Recall@k**
Measure how often the relevant documents are retrieved.

#### **Implementation**
```python
def evaluate_recall_at_k(retriever, query, ground_truth_docs, k=5):
    retrieved_docs = retriever.similarity_search(query, k=k)
    retrieved_content = [doc.page_content for doc in retrieved_docs]
    recall = len(set(retrieved_content) & set(ground_truth_docs)) / len(ground_truth_docs)
    print(f"Recall@{k}: {recall:.2f}")
    return recall

# Example Usage
query = "What is quantum mechanics?"
ground_truth_docs = ["Quantum mechanics is a fundamental theory in physics."]
evaluate_recall_at_k(retriever, query, ground_truth_docs)
```

---

## **5. Advanced Debugging**

### **5.1 Visualize Embeddings**
Use tools like **t-SNE** or **UMAP** to visualize embeddings and detect clustering issues.

#### **Implementation**
```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def visualize_embeddings(embeddings, labels):
    tsne = TSNE(n_components=2)
    reduced = tsne.fit_transform(embeddings)
    plt.scatter(reduced[:, 0], reduced[:, 1])
    for i, label in enumerate(labels):
        plt.annotate(label, (reduced[i, 0], reduced[i, 1]))
    plt.show()

# Example Usage
query_embedding = embedding_model.encode("What is quantum mechanics?")
doc_embeddings = [embedding_model.encode(doc.page_content) for doc in retrieved_docs]
visualize_embeddings([query_embedding] + doc_embeddings, ["Query"] + [f"Doc {i+1}" for i in range(len(retrieved_docs))])
```

---

## **6. Conclusion**

By implementing offline evaluation techniques, real-time monitoring, and advanced debugging methods, you can systematically identify and address issues in your RAG model's responses. These practices ensure the model delivers accurate, reliable, and grounded answers.

